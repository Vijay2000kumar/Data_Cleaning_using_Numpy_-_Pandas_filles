# -*- coding: utf-8 -*-
"""End_to _End_datacleaning using Linkedin_job_posting dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18Yl2RZgDe4sU0AP8E1XZGXzdkOsKX10s

### 1.Data Summary


---


The LinkedIn data set provided contains 7,927 rows and 15 columns, providing a comprehensive overview of job postings on the platform. The data can be used for data analysis, visualization, and research. The job postings include Data Analyst, Machine Learning Engineer, IT Services and IT Consulting roles, located in various locations around the world, with varying salaries and work hours. The data set includes information about the company, role responsibilities, and required skills for each job. This data set is a valuable resource for understanding job opportunities in different industries and locations.

### 2.Column descriptions

`job_ID`: Unique identifier for each job posting.

`job`: The title of the job posting.

`location`: The location of the job posting.

`company_id`: The unique identifier for the company offering the job.

`company_name`: The name of the company offering the job.

`work_type`: The type of work offered (e.g. full-time, part-time, etc.).

`full_time_remote`: Indicates if the job is a full-time remote position.

`no_of_employ`: The number of employees at the company offering the job.

`no_of_application`: The number of applications received for the job.

`posted_day_ago`: The number of days ago the job was posted.

`alumni`: Indicates if the job posting is for alumni of a certain organization.

`Hiring_person`: The name of the person responsible for hiring for the job.

`linkedin_followers`: The number of LinkedIn followers of the hiring person.

`hiring_person_link`: A link to the LinkedIn profile of the hiring person.

`job_details`: Detailed information about the job, including responsibilities and requirements.

## Issue with the data set


1.   Dirty Data:-
  - **job**: column in the data set there is unnecessary information such as the yearly package, technology, company name, and work type (remote) etc added in the title.`Consistency`
  - **no_of_application**: this column the has a value which should not be present ('hours' , 'minutes'). `Consistency`
  - **posted_day_ago**: In this column time is added in the form of string for Exampal (9 hours , 8 minutes). `Validity`
  - **Alumni**: with the alumni count the string is present with the count value ('company alumni'). `Validity`
  - **company_id**: This column is completly blank. `Completeness`
  - **Hiring_person**: In this column there are some nick name is added with in parentheses `Consistency`
  - **linkedin_followers**: In linkedin_followers column the 'linkedin_followers' string is present with numbers and numbers is seprated with (' , ') `Validity`





2.   Messy Data:-
  
  - **Full_time_remote**: In this column involvement and level both combiened in one column and seprated by ('·'). `Validity`
  - **No_of_employ**: In this column employees_count and industry both combiened in one column and 'employees' string is added extra seprated ('·'). `Validity`
  - **location**: In the location column countory,state and city is added in the one column and seprated by (',') `Validity`
  - **Job**: In the Job column some are starting with the capital letter and some are in small letter `Consistency`
  -**company_name**: in the company_name also there is inconsestancy in the name some of the letters are starting with the small letter and some of them are stares with the capital letter. `Consistency`
  - There are 2,084 dublicate values found in the dataset `Validity`

### Exploratory Data Analysis (EDA) Functions in Python.

- head and tail
- sample
- info
- isnull
- duplicated
- describe

**Importing the required librarys**
"""

import pandas as pd
import numpy as np
import warnings

warnings.filterwarnings("ignore", category=UserWarning)

"""**Loading The Data Into The Pandas DataFrame**"""

df=pd.read_csv("/content/linkdin_Job_data.csv")

"""**Cheking Top 5 Rows**"""

df.head()

"""**Cheaking The Last 5 Rows**"""

df.tail()

"""**Cheaking 5 Ramdon Rows**"""

df.sample(5)

"""## Data Cleaning

**Cheaking The Null Value Presant In The Dataframe**
"""

df.isnull().sum()

"""**Column-wise null values percentage**"""

df.isnull().sum()/df.shape[0]*100

"""**Cheacking the data type and non null value present in the dataframe**"""

df.info()

"""**Cheking the dublicate value presant in the dataset**"""

df[df.duplicated(subset=['job_ID'])].count()

"""### Data Quality Dimensions

- Completeness -> is data missing?
- Validity -> is data invalid -> negative height -> duplicate patient id
- Accuracy -> data is valid but not accurate -> weight -> 1kg
- Consistency -> both valid and accurate but written differently -> New Youk and NY

#### Steps involved in Data cleaning
- Define
- Code
- Test

`Always make sure to create a copy of your pandas dataframe before you start the cleaning process`

**Creating a copy of dataframe**
"""

job_df=df.copy()

"""### Define
- removing dublicate values from the dataframe
-replace the null values or remove it based on analysis
- company_id is a blank column So, Assigning a unique identifier for each company based on their order of appearance in the DataFrame

**Checking duplicate values**
"""

job_df[df.duplicated(subset=['job_ID'])]

"""**Removing the duplicates values and reset the index values**"""

job_df.drop_duplicates(subset=['job_ID'],inplace=True)
job_df.reset_index(inplace=True)
job_df.drop('index',axis=1,inplace=True)

"""**After removing the duplicates dataframe shape**"""

job_df.shape

"""**Assigns a unique numerical identifier to each company in the 'company_name' column and adds it in a 'company_id' column
based on the company's order of appearance in the DataFrame.**
"""

job_df['company_id']=job_df.groupby('company_name').ngroup() + 1

"""**Assigns a unique numerical identifier to each company in the 'job_ID' column and adds it in a 'details_id' column
based on the Job's order of appearance in the DataFrame.**
"""

# Assign a unique identifier to each job in the job dataframe
job_df['details_id'] = job_df.groupby('job_ID').ngroup() + 1

job_df.head(1)

"""**Drop The unnessesary columns**"""

job_df.drop(['alumni','hiring_person_link'],axis=1,inplace=True)

job_df.head()

"""### **Handling missing values**
  - All the column has the null value except `job_ID` and `company_id`
  - `job` has **27** Null values -- This column has the less null value we can remove all the null values
  - `location` has **27** Null values -- This column also has less null value so we can directly remove all
  - `company_name` has **28** Null values -- This column also has less null value so we can directly remove all
  - `work_type` has **132** Null values -- Since the data in this column object type we can replace it with the mode
  -`full_time_remote` has **67** Null values -- This column also has less null value so we can directly remove all
  - `no_of_employ` has **229** Null values -- This column also has less null value so we can directly remove all
  -`no_of_application` has **33** Null values -- either we can remove it or replace it with the 0 because i know for a fact there is null values because at that time there is no application
  -`posted_day_ago` has **6** Null values -- This column also has less null value so we can directly remove all
  - `Hiring_person` has **1686** Null values -- In this we are going to replace the null values with 'Not Available'
  -`linkedin_followers` has **2313** -- In this column there is a posiblity there is 0 followers so null is placed insist of it but i am going to replace it with the mean.
  -`job_details` has **44** Null values -- This column also has less null value so we can directly remove all

**Cheaking the null values**
"""

job_df.isnull().sum()

"""**Columns wise null percentage**"""

job_df.isnull().sum()/job_df.shape[0]*100

"""**Removing unwanted srings from the dataFrame**"""

job_df=job_df.replace({'company alumni':'','followers':'','employees':''},regex=True)
job_df[['job','no_of_employ','linkedin_followers']]=job_df[['job','no_of_employ','linkedin_followers']].replace({',':''},regex=True)
job_df['linkedin_followers'] = pd.to_numeric(job_df['linkedin_followers'], errors='coerce')

"""**Remove the null values**"""

job_df.dropna(subset=['job','no_of_employ','location','company_name','full_time_remote','posted_day_ago','job_details'],inplace=True)
job_df.work_type.fillna(df.work_type.mode()[0],inplace=True)
job_df.Hiring_person.fillna('Not Available',inplace=True)
job_df.linkedin_followers.fillna(job_df.linkedin_followers.mean(),inplace=True)
job_df.isnull().sum()

"""### Define


1.   Validity
    - In the Posted_day_ago there is inconsistency in the units some are in minutes and some of them in hours. So, converting them into the same unit `create seprate a funtion`
  - In the Alumni column removing the string ("company alumni") because we want to convert it intoo the int data type  `by df.col.str.replace() or df.replace()`
  - In linkedin_followers column we are also going to remove unnecessary stings and removing the (',') `by df.col.str.replace() or df.replace()`
  - In Full_time_remote column we are going to seprate the involvement and level in each individual column with the healp of seprator('·') `by  split()`
    - In No_of_employ column we are going to seprate the employees_count and industry in each individual column with the help of seprator('·') `by split()`
  - In the location column there is three individual values so going to seprate them into three columns country,state,city `by split()`
"""

def convert_posted_day_ago_to_minutes(df):
    # Filter the DataFrame to only include rows where "posted_day_ago" is not null
    df = df[df['posted_day_ago'].notnull()]

    # Use regular expressions to extract the numeric value and the time unit (hours, minutes, days, weeks, or months)
    regex_pattern = r'(\d+)\s+(hour|minute|day|week|month)s?'
    matches = df['posted_day_ago'].str.extract(regex_pattern, expand=True)

    # Convert the numeric values to integers and multiply by the appropriate factor for each time unit
    # Factor for hours: 60
    # Factor for minutes: 1
    # Factor for days: 1440 (24 * 60)
    # Factor for weeks: 10080 (7 * 24 * 60)
    # Factor for months: 43800 (30 * 24 * 60)
    factor = matches[1].replace({'hour': 60, 'minute': 1, 'day': 1440, 'week': 10080, 'month': 43800})
    df['posted_day_ago'] = matches[0].astype(float) * factor.astype(float)

    return df

# converting all the value in the same unit minutes
job_df.posted_day_ago.dropna(inplace=True)
job_df=convert_posted_day_ago_to_minutes(job_df)

# cheaking
job_df.head()

"""### Saprating the columns"""

job_df.no_of_employ

# Split 'no_of_employ' column on '·' separator and store in new columns
job_df[['no_of_employ', 'company_sector']] = pd.DataFrame(job_df['no_of_employ'].str.split('·').tolist(), index=job_df.index)

# Split 'full_time_remote' column on '·' separator and store in new columns
job_df[['full_time_remote', 'Job_level']] = pd.DataFrame(job_df['full_time_remote'].str.split('·').tolist(), index=job_df.index)

#Testing
job_df.head()

# checking the null value
job_df.isnull().sum()

"""#### Handling the null value gentared after seprating the column
- `company_sector` has 1957 Null value --  In this column we are replacing with the 'Not Avilable
- `Job_level` has 2070 Null value -- In this column we are replacing with the 'Not Avilable'
- `posted_day_ago` has 3 Null value -- Removing all the null value
"""

# code

job_df.company_sector.fillna('Not Avilable',inplace=True)
job_df.Job_level.fillna('Not Avilable',inplace=True)
job_df.dropna(subset=['posted_day_ago'],inplace=True)

# test

job_df.isnull().sum()

job_df.location[job_df.location.str.split().str.len()==6]

"""### Seprating the city and state from `location`"""

# seprating the state
job_df['City']=job_df['location'][(job_df.location !='India')].str.split(',').str[0]

"""#### Create a function to convert citys to states"""

def find_state(x):
    """
    Returns the state corresponding to the given area.

    Parameters:
        x (str): The name of the area.

    Returns:
        str: The corresponding state.

    """
    if x == 'Greater Bengaluru Area':
        return 'Karnataka'
    elif x == 'Mumbai Metropolitan Region':
        return 'Maharashtra'
    elif x == 'Greater Kolkata Area':
        return 'West Bengal'
    elif x == 'Pune/Pimpri-Chinchwad Area':
        return 'Maharashtra'
    elif x == 'Greater Hyderabad Area':
        return 'Telangana'
    elif x == 'Greater Delhi Area':
        return 'Delhi'
    elif x == 'Greater Coimbatore Area':
        return 'Tamil Nadu'
    elif x == 'Greater Chennai Area':
        return 'Tamil Nadu'
    elif x == 'Greater Ahmedabad Area':
        return 'Gujarat'
    elif x == 'Greater Nagpur Area':
        return 'Maharashtra'
    else:
        return 'error'

"""#### Install the geopy module"""

!pip install geopy -q

"""#### Create a function to find the states with the help of citys"""

from geopy.geocoders import Nominatim
from functools import lru_cache
geolocator = Nominatim(user_agent="apps")
@lru_cache(maxsize=None)
def extract_state(x):
    try:
#         geocode = lambda query, **kw: _geocode("%s, Cleveland OH" % query, **kw)
        location = geolocator.geocode(x,addressdetails=True)
        return location.raw['address']['state']
    except Exception as e:
        print(e)
        return find_state(x)

"""#### Create a function to find the City with the help of citys"""

from geopy.geocoders import Nominatim
from functools import lru_cache
geolocator = Nominatim(user_agent="apps")
@lru_cache(maxsize=None)
def extract_city(x):
    try:
#         geocode = lambda query, **kw: _geocode("%s, Cleveland OH" % query, **kw)
        location = geolocator.geocode(x,addressdetails=True)
        return location.raw['address']['city']
    except Exception as e:
#         print(e)
        return np.nan

"""#### Applying the extract_state function to the dataframe `state` column"""

job_df['State']=job_df['City'].apply(extract_state)

job_df[['State']].replace({'Trentino-Alto Adige/Südtirol':np.nan},inplace=True)

"""**we are getting some error during applying the above fuction.
So, getting the missing values from the location column**
"""

job_df['State'][job_df['State']=='error']

print(job_df['State'][job_df['State']=='error'].count())

job_df['State'][job_df['State']=='error']=job_df['location'][job_df['State']=='error'].str.split(',').str[1]

"""**Checking the noise values present in the City column**"""

job_df['City'].unique()

"""**Now used extract_city function on a `City` Column to get the Delhi city.**"""

job_df['City'][job_df['City'].isnull()]=job_df['State'][job_df['City'].isnull()].apply(extract_city)

"""#### There are `42` states are presant in the city column"""

job_df[['City','State']][(job_df.City==job_df.State)&(job_df.City!='Delhi')].count()

"""**Replacing those city cells in which state names stored**"""

job_df['City'][(job_df.City==job_df.State)&(job_df.City!='Delhi')]=np.nan

"""**There are `538` Null value are present in the city column**"""

job_df[['State','City']].isnull().sum()

"""**Checking the noise values present in the State column**"""

job_df.State.unique()

"""**In the State column instead of null values 'Trentino-Alto Adige/Südtirol' is presant**
- replacing it with the null values
"""

job_df.State[job_df['City'].isnull()]

job_df['State'] = job_df['State'].str.strip().replace({'Trentino-Alto Adige/Südtirol': np.nan})

job_df.head()

"""There is nosise present in the job column into the brode category for example 'Analytics Manager','Security_Analyst_GRC' converted to 'Data Analyst'"""

job_df.loc[job_df['job'].str.contains('data engineer|Data Enginer|Talend Developer|ETL Developer|Informatica Developer',case=False), 'job'] = 'Data Engineer'
job_df.loc[job_df['job'].str.contains('data |Analytics Manager|Security_Analyst_GRC|Conversion Analyst|Analytics|Data Analytics|Data Analysis|Senior Analyst|Senior Research Analyst',case=False), 'job'] = 'Data Analyst'
job_df.loc[job_df['job'].str.contains('business analyst|Strategy Analyst|Business Intelligence Analyst|Business System Analyst|Operations Analyst',case=False), 'job'] = 'Business Analyst'
job_df.loc[job_df['job'].str.contains('database',case=False), 'job'] = 'Database Developer'
job_df.loc[job_df['job'].str.contains('cloud',case=False), 'job'] = 'Cloud Engineer'
job_df.loc[job_df['job'].str.contains('python',case=False), 'job'] = 'Python Developer'
job_df.loc[job_df['job'].str.contains('angular',case=False), 'job'] = 'Angular Developer'
job_df.loc[job_df['job'].str.contains('blockchain developer|Metaverse|blockchain',case=False), 'job'] = 'Blockchain Developer'
job_df.loc[job_df['job'].str.contains('data science|Data Science|Senior Applied Scientist – Learning Analytics',case=False), 'job'] = 'Data Science'
job_df.loc[job_df['job'].str.contains(r'\bML\b|machine learning',case=False), 'job']='Machine Learning Engineer'
job_df.loc[job_df['job'].str.contains(r'\bAI\b|Artificial Intelligence Engineer',case=False), 'job']='AI Engineer'
job_df.loc[job_df['job'].str.contains('sql',case=False), 'job']='SQL Developer'
job_df.loc[job_df['job'].str.contains('bigdata|Big Data',case=False), 'job']='Bigdata Developer'
job_df.loc[job_df['job'].str.contains('linux',case=False), 'job']='Linux Engineer'
job_df.loc[job_df['job'].str.contains('java full stack',case=False), 'job']='Java Full Stack Develope'
job_df.loc[job_df['job'].str.contains('java software engineer|AEM Developer|AEM-developer',case=False), 'job']='Java Software Engineer'
job_df.loc[job_df['job'].str.contains('java',case=False), 'job']='Java Developer'
job_df.loc[job_df['job'].str.contains('project manager|Product Manager',case=False), 'job']='Project Manager'
job_df.loc[job_df['job'].str.contains('azure|devops|AWS Engineer|Build Engineer|GCP Engineer|DevSecOps',case=False), 'job']='DevOps Engineer'
job_df.loc[job_df['job'].str.contains('PHP|Laravel Developer',case=False), 'job']='PHP Developer'
job_df.loc[job_df['job'].str.contains('game',case=False), 'job']='Game Developer'
job_df.loc[job_df['job'].str.contains(('android|mobile application'),case=False), 'job']='Mobile Application Developer'
job_df.loc[job_df['job'].str.contains('automation tester|Automation',case=False), 'job']='Automation Tester'
job_df.loc[job_df['job'].str.contains('quality analyst',case=False), 'job']='Quality Analyst'
job_df.loc[job_df['job'].str.contains('ruby',case=False), 'job']=' Ruby on Rails Developer'
job_df.loc[job_df['job'].str.contains('team lead',case=False), 'job']='Technical Team Lead'
job_df.loc[job_df['job'].str.contains('.net',case=False), 'job']='.Net Developers'
job_df.loc[(job_df['job'].str.contains('quality', case=False)) & ((job_df.job != 'Data Analyst') & (job_df.job != 'Business Analyst')), 'job'] = 'Quality Analyst'
job_df.loc[job_df['job'].str.contains('reactjs|React|React.JS|Senior React Developer',case=False), 'job']='ReactJS Developer'
job_df.loc[job_df['job'].str.contains('human resources|recruiter',case=False), 'job']='Human Resources Intern'
job_df.loc[job_df['job'].str.contains('test automation',case=False), 'job']='Automation Tester'
job_df.loc[job_df['job'].str.contains('salesforce',case=False), 'job']='Salesforce Developer'
job_df.loc[job_df['job'].str.contains('business development|Senior Manager Analytics',case=False), 'job']='Business Development Manager'
job_df.loc[job_df['job'].str.contains('lead developer',case=False), 'job']='Lead developer'
job_df.loc[job_df['job'].str.contains('front-end',case=False), 'job']='Front-End Developer'
job_df.loc[job_df['job'].str.contains('c\+\+',case=False), 'job']='C++ Developer'
job_df.loc[job_df['job'].str.contains('wordpress',case=False), 'job']='Wordpress Developer'
job_df.loc[job_df['job'].str.contains('software engineer|Algorithm Engineer|Snaplogic Developer|Visual Basic Developer|AS400|Virtusa is hiring for VAX/VMS|Urgent opening for Pega CDH - RTIM',case=False), 'job']='Software Engineer'
job_df.loc[job_df['job'].str.contains(r'\bc developer\b',case=False), 'job']='C Developer'
job_df.loc[job_df['job'].str.contains('full stack',case=False), 'job']='Full Stack Developer'
job_df.loc[job_df['job'].str.contains('golang',case=False), 'job']='Golang Developer'
job_df.loc[job_df['job'].str.contains('shopify',case=False), 'job']='Shopify Developer'
job_df.loc[job_df['job'].str.contains('data scientist|Computer Vision|Data Science',case=False), 'job']='Data Scientist'
job_df.loc[job_df['job'].str.contains('snowflake',case=False), 'job']='Snowflake Developer'
job_df.loc[job_df['job'].str.contains('mainframe',case=False), 'job']='Mainframe Developer'
job_df.loc[job_df['job'].str.contains(r'\bSEO\b|search engine|SEO Analyst|Google Analytics|eCommerce',case=False), 'job']='Search Engine Optimization'
job_df.loc[job_df['job'].str.contains('unity',case=False), 'job']='Unity Developer'
job_df.loc[job_df['job'].str.contains('ios|Apple App Developer',case=False), 'job']='iOS Developer'
job_df.loc[job_df['job'].str.contains(r'\bsales\b|Telesales|Executive', case=False), 'job']='Sales Executive'
job_df.loc[job_df['job'].str.contains(r'\bBI\b',case=False), 'job']='Power BI Developer'
job_df.loc[job_df['job'].str.contains('customer|Voice Process',case=False), 'job']='Customer Service'
job_df.loc[job_df['job'].str.contains('Scrum Master',case=False), 'job']='Scrum Master'
job_df.loc[job_df['job'].str.contains('drupal',case=False), 'job']='Drupal Developer'
job_df.loc[job_df['job'].str.contains('digital marketing',case=False), 'job']='Digital Marketing'
job_df.loc[job_df['job'].str.contains('Back End Developer|Perl|MERN|Backend|Lead Back-end Developer|Mern stack|Mean Stack',case=False), 'job']='BackEnd Developer'
job_df.loc[job_df['job'].str.contains('Relationship',case=False), 'job']='Relationship Manager'
job_df.loc[job_df['job'].str.contains('node',case=False), 'job']='Node Js Developer'
job_df.loc[job_df['job'].str.contains('Account|Experienced CA Fully qualified + Statutory Audit and Reporting with IFRS & GAAP (1-3Years) - Senior Analyst - Finance',case=False), 'job']='Accountant'
job_df.loc[job_df['job'].str.contains('Web',case=False), 'job']='Web Developer'
job_df.loc[job_df['job'].str.contains('Architect|Technical Lead',case=False), 'job']='Technology Architecture'
job_df.loc[job_df['job'].str.contains('Frontend|Graphic Designer',case=False), 'job']='Frontend Developer'
job_df.loc[job_df['job'].str.contains('Copywriter|Technical Editor',case=False), 'job']='Copywriter'
job_df.loc[job_df['job'].str.contains('Tester|Test',case=False), 'job']='Software Testing'
job_df.loc[job_df['job'].str.contains('Consultant',case=False), 'job']='Consultant'
job_df.loc[job_df['job'].str.contains('SAP',case=False), 'job']='SAP Developer'
job_df.loc[job_df['job'].str.contains('Vue',case=False), 'job']='Vue.js Developer'
job_df.loc[job_df['job'].str.contains('Intern|Volunteers',case=False), 'job']='Internships'
job_df.loc[job_df['job'].str.contains(r'\bdata\b', case=False) & ~job_df['job'].str.contains('data Analyst|data engineer|Data Scientist|Data Science', case=False), 'job']='Data Management'
job_df.loc[job_df['job'].str.contains('Quantitative',case=False), 'job']='Quantitative Trader'
job_df.loc[job_df['job'].str.contains('Finance|Senior Manager Risk Management|Pricing Manager|Tax Manager|Assistant Manager|Assistant Vice President',case=False), 'job']='Finance Manager'
job_df.loc[job_df['job'].str.contains('Professor',case=False), 'job']='Professor'
job_df.loc[job_df['job'].str.contains('Robotics',case=False), 'job']='Robotics'
job_df.loc[job_df['job'].str.contains(r'\bAnalyst\b', case=False) & ~job_df['job'].str.contains('data Analyst|Business Analyst|Quality Analyst', case=False), 'job']='Analyst'
job_df.loc[job_df['job'].str.contains('Teacher|Academic|Student|Educational Counselor|Onboarding Specialis|Academic Counsellor|Counsellor', case=False),'job']='Education and Counseling'
job_df.loc[job_df['job'].str.contains('Oracle', case=False),'job']='Oracle Developer'
job_df.loc[job_df['job'].str.contains('PowerApps|Power App', case=False),'job']='PowerApps Developer'
job_df.loc[job_df['job'].str.contains(r'\bC\b', case=False),'job']='C Developer'
job_df.loc[job_df['job'].str.contains(r'\bMarketing\b', case=False) & ~job_df['job'].str.contains('Digital Marketing', case=False), 'job']='Marketing'
job_df.loc[job_df['job'].str.contains('Developer|Programmer', case=False) & ~job_df['job'].str.contains('Blockchain Developer|Salesforce Developer|ReactJS Developer|Net Developers|Ruby on Rails Developer|Mobile Application Developer|Game Developer|PHP Developer|Java Developer|Bigdata Developer|SQL Developer|Angular Developer|Python Developer|Database Developer|C Developer|PowerApps Developer|Oracle Developer|Vue.js Developer|SAP Developer|Frontend Developer|Web Developer|Node Js Developer|BackEnd Developer|Drupal Developer|Power BI Developer|iOS Developer|Unity Developer|Mainframe Developer|Snowflake Developer|Shopify Developer|Golang Developer|(Full Stack|C|Wordpress|C\+\+|Front-End|Lead) Developer', case=False), 'job']='Other Developer'
job_df.loc[job_df['job'].str.contains('Support', case=False),'job']='Support Engineer'
job_df.loc[job_df['job'].str.contains('System|IMS Engineer|Systems Engineer (BSW)|Solarwind Development Engineer|Site Reliability Engineer (SRE)|Platform Engineer|Infrastructure Engineer|UCC Engineer Documentation|Integration Engineer', case=False),'job']='System Engineer'
job_df.loc[job_df['job'].str.contains('Engineer', case=False) & ~job_df['job'].str.contains('Software Engineer|DevOps Engineer|Java Software Engineer|Linux Engineer|AI Engineer|Machine Learning Engineer|Cloud Engineer|Data Engineer', case=False), 'job']='Other Engineering'
job_df.loc[job_df['job'].str.contains('Manager|Leadership', case=False) & ~job_df['job'].str.contains('Business Development Manager|Finance Manager|Relationship Manager|Project Manager', case=False), 'job']='Managerial and Leadership Roles'
job_df.loc[job_df['job'].str.contains(r'\bLead\b', case=False) & ~job_df['job'].str.contains('Lead developer', case=False), 'job']='Team Lead/ Project Lead'
job_df.loc[job_df['job'].str.contains('Editor', case=False),'job']='Editor'
job_df.loc[job_df['job'].str.contains('Associate', case=False),'job']='Associate'
job_df.loc[job_df['job'].str.contains('Research', case=False),'job']='Researcher'
job_df.loc[job_df['job'].str.contains('Financial', case=False),'job']='Financial Controller'
job_df.loc[job_df['job'].str.contains('Head of', case=False),'job']='Head of Product'
job_df.loc[job_df['job'].str.contains('Technical', case=False),'job']='Technical Trainer/Writer/Owner'
job_df.loc[job_df['job'].str.contains('Storage', case=False),'job']=' Storage Administrator'

"""**We convert all job titles into others for those individual titles has value_count() `1`**"""

job_counts = job_df['job'].value_counts()
unique_jobs = job_counts[job_counts == 1].index
job_df.loc[job_df['job'].isin(unique_jobs), 'job'] = 'Other'

"""**After optimizing the job title there is 84 unique jobs are available**"""

job_df.job.nunique()

job_df.sample(n=5)

job_df.job.value_counts()

"""#### removing the '+' symbol from the `no_of_employ` column"""

job_df['no_of_employ']=job_df['no_of_employ'].str.replace('+','',regex=True)

job_df.head()

"""#### Assigning lower side to  `no_of_employ` and convert it into int data type"""

job_df['no_of_employ']=job_df['no_of_employ'].str.split('-').str[0]
job_df['no_of_employ']=job_df['no_of_employ'].astype(int)

job_df.rename(columns={"job": "designation",'company_name':'name','company_sector':'industry','no_of_employ':'employees_count','full_time_remote':'involvement','Job_level':'level','no_of_application':'total_applicants'}, inplace=True)

"""**Removing extra space**"""

job_df.involvement=job_df.involvement.replace({' ':'','11-50':np.nan,'1-10':np.nan},regex=True)

job_df.involvement.unique()

job_df.isnull().sum()

"""**Null percentage of the DataFrame Columns**"""

round(job_df.isnull().sum()/job_df.shape[0]*100,2)[round(job_df.isnull().sum()/job_df.shape[0]*100,2)>0]

"""**Cheaking most for frequant value for each state.**

In the sate column there is 3 wrong States are there.`Apac`,`Auvergne-Rhône-Alpes`,`রংপুর বিভাগ`
- In the sate colomn `Auvergne-Rhône-Alpes` is there insted of Gujarat
"""

a=job_df.groupby('State')[['City']].agg(pd.Series.mode).reset_index()
b = dict(zip(a['State'], a['City']))

job_df[['State','City']][job_df.State=='Auvergne-Rhône-Alpes'].head()

"""**Replacing the `Auvergne-Rhône-Alpes` with `Gujarat`**"""

job_df.State=job_df.State.str.replace('Auvergne-Rhône-Alpes','Gujarat')

"""- checking how mamy rows are there in which`['Apac', 'রংপুর বিভাগ']` are in state column
    - there are very less rows are there with the above condition.
    - So removed them from the dataframe
"""

job_df[['State','City']][job_df['State'].isin(['Apac', 'রংপুর বিভাগ'])]

job_df=job_df[~job_df['State'].isin(['Apac', 'রংপুর বিভাগ'])]

"""- Removing the NAN value from involvement column
- Replacing NAN Values from `State` and `City` columns with Mode
"""

job_df.dropna(subset='involvement',inplace=True)

job_df.State.fillna(job_df.State.mode()[0],inplace=True)

"""- Creating the funtion for replacing the null values for the city column"""

def fillnull_city(a):
    if type(a)==str:
        return str(b[a])
    else:
        return 'NAN'

job_df['City'][job_df.City.isnull()]=job_df['State'][job_df.City.isnull()].apply(fillnull_city)

job_df.City.isnull().sum()

job_df.head()

for i in job_df:
    print(i,job_df[i].unique())
    print('--------------------------------------')

job_df.total_applicants[job_df['total_applicants'].isin(['minutes','hours','hour','minute','day','days'])]=0

job_df.drop(['location','posted_day_ago','Hiring_person'],axis=1,inplace=True)

job_df.total_applicants.unique()

job_df.total_applicants = pd.to_numeric(job_df.total_applicants, errors='coerce')

job_df.to_csv('c:\\Project\\Linkdin_Job_Analytics\\DataSet\\job_cleanData.csv',index=False)